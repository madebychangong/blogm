"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ - ìˆœìˆ˜ ë°ì´í„° ìˆ˜ì§‘ ì „ìš©
ë¶„ì„ ë¡œì§ ì—†ì´ ë¹ ë¥´ê²Œ raw ë°ì´í„°ë§Œ ìˆ˜ì§‘
ë¹„ë™ê¸° HTTP ìš”ì²­ìœ¼ë¡œ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì§€ì›
"""
import re
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import json


class NaverBlogCrawler:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬"""

    def __init__(self, max_posts: int = 100, timeout: int = 15):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36',
            'Referer': 'https://blog.naver.com/',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        self.timeout = timeout
        self.max_posts = max_posts

    async def crawl_blog(self, blog_id: str, save_to_file: bool = False) -> Dict:
        """
        ë¸”ë¡œê·¸ ì „ì²´ í¬ë¡¤ë§

        Args:
            blog_id: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ID
            save_to_file: JSON íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€

        Returns:
            í¬ë¡¤ë§ëœ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬)
        """
        print(f"ğŸš€ [{blog_id}] í¬ë¡¤ë§ ì‹œì‘...")

        # 1. ê²Œì‹œê¸€ URL ìˆ˜ì§‘
        post_urls = await self._collect_post_urls(blog_id)
        if not post_urls:
            print(f"âŒ [{blog_id}] ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print(f"âœ… [{blog_id}] {len(post_urls)}ê°œ ê²Œì‹œê¸€ URL ìˆ˜ì§‘ ì™„ë£Œ")

        # 2. ê° ê²Œì‹œê¸€ ë³‘ë ¬ í¬ë¡¤ë§
        async with aiohttp.ClientSession(headers=self.headers) as session:
            tasks = [
                self._crawl_single_post(session, url, blog_id)
                for url in post_urls[:self.max_posts]
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        posts = []
        for result in results:
            if result and not isinstance(result, Exception):
                posts.append(result)

        print(f"âœ… [{blog_id}] {len(posts)}/{len(post_urls)} ê²Œì‹œê¸€ í¬ë¡¤ë§ ì™„ë£Œ")

        # 3. ë¸”ë¡œê·¸ ì •ë³´ ìˆ˜ì§‘
        blog_info = await self._get_blog_info(blog_id)

        # 4. ìµœì¢… ê²°ê³¼
        result = {
            "blog_id": blog_id,
            "blog_info": blog_info,
            "total_posts": len(posts),
            "posts": posts,
            "crawled_at": datetime.now().isoformat()
        }

        # 5. íŒŒì¼ ì €ì¥
        if save_to_file:
            filename = f"{blog_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ [{blog_id}] ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")

        return result

    async def _fetch(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """ë¹„ë™ê¸° HTTP ìš”ì²­"""
        try:
            async with session.get(
                url,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"âš ï¸ HTTP {response.status}: {url}")
                    return None
        except asyncio.TimeoutError:
            print(f"â±ï¸ Timeout: {url}")
            return None
        except Exception as e:
            print(f"âŒ Error: {url} - {e}")
            return None

    async def _collect_post_urls(self, blog_id: str) -> List[str]:
        """ê²Œì‹œê¸€ URL ìˆ˜ì§‘ (ë‹¤ì¤‘ í´ë°±)"""
        async with aiohttp.ClientSession(headers=self.headers) as session:
            urls = []

            # ë°©ë²• 1: RSS (ê°€ì¥ ì•ˆì •ì )
            print(f"ğŸ“¡ [{blog_id}] RSSë¡œ URL ìˆ˜ì§‘ ì‹œë„...")
            urls = await self._collect_from_rss(session, blog_id)
            if urls:
                return urls

            # ë°©ë²• 2: PC ë©”ì¸ â†’ iframe
            print(f"ğŸ“¡ [{blog_id}] PC ë©”ì¸í˜ì´ì§€ë¡œ URL ìˆ˜ì§‘ ì‹œë„...")
            urls = await self._collect_from_main(session, blog_id)
            if urls:
                return urls

            # ë°©ë²• 3: PostList API
            print(f"ğŸ“¡ [{blog_id}] PostList APIë¡œ URL ìˆ˜ì§‘ ì‹œë„...")
            urls = await self._collect_from_postlist(session, blog_id)
            if urls:
                return urls

            # ë°©ë²• 4: ëª¨ë°”ì¼ í˜ì´ì§€
            print(f"ğŸ“¡ [{blog_id}] ëª¨ë°”ì¼ í˜ì´ì§€ë¡œ URL ìˆ˜ì§‘ ì‹œë„...")
            urls = await self._collect_from_mobile(session, blog_id)

            return urls

    async def _collect_from_rss(self, session: aiohttp.ClientSession, blog_id: str) -> List[str]:
        """RSSì—ì„œ URL ìˆ˜ì§‘"""
        try:
            rss_url = f"https://rss.blog.naver.com/{blog_id}.xml"
            html = await self._fetch(session, rss_url)
            if not html:
                return []

            soup = BeautifulSoup(html, 'xml')
            urls = []

            for item in soup.find_all('item'):
                link = item.find('link')
                if link and link.text:
                    # logNo ì¶”ì¶œ
                    log_nos = re.findall(r'logNo=(\d+)', link.text)
                    if not log_nos:
                        log_nos = re.findall(rf'/{re.escape(blog_id)}/(\d+)', link.text)

                    if log_nos:
                        urls.append(f"https://blog.naver.com/{blog_id}/{log_nos[0]}")

                if len(urls) >= self.max_posts:
                    break

            return urls
        except Exception as e:
            print(f"âŒ RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _collect_from_main(self, session: aiohttp.ClientSession, blog_id: str) -> List[str]:
        """PC ë©”ì¸í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘"""
        try:
            main_url = f"https://blog.naver.com/{blog_id}"
            html = await self._fetch(session, main_url)
            if not html:
                return []

            soup = BeautifulSoup(html, 'html.parser')
            iframe = soup.find('iframe', id='mainFrame')

            if iframe and iframe.get('src'):
                inner_url = iframe['src']
                if inner_url.startswith('/'):
                    inner_url = 'https://blog.naver.com' + inner_url

                inner_html = await self._fetch(session, inner_url)
                if inner_html:
                    return self._extract_urls_from_html(inner_html, blog_id)

            return []
        except Exception as e:
            print(f"âŒ ë©”ì¸í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _collect_from_postlist(self, session: aiohttp.ClientSession, blog_id: str) -> List[str]:
        """PostList APIì—ì„œ URL ìˆ˜ì§‘"""
        try:
            url = f"https://blog.naver.com/PostList.naver?blogId={blog_id}&widgetTypeCall=true&directAccess=true"
            html = await self._fetch(session, url)
            if html:
                return self._extract_urls_from_html(html, blog_id)
            return []
        except Exception as e:
            print(f"âŒ PostList ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _collect_from_mobile(self, session: aiohttp.ClientSession, blog_id: str) -> List[str]:
        """ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘"""
        try:
            url = f"https://m.blog.naver.com/{blog_id}"
            html = await self._fetch(session, url)
            if html:
                return self._extract_urls_from_html(html, blog_id)
            return []
        except Exception as e:
            print(f"âŒ ëª¨ë°”ì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def _extract_urls_from_html(self, html: str, blog_id: str) -> List[str]:
        """HTMLì—ì„œ ê²Œì‹œê¸€ URL ì¶”ì¶œ"""
        urls = []
        patterns = [
            rf'/{re.escape(blog_id)}/(\d+)',
            r'logNo=(\d+)',
            r'data-log-no=["\'](\d+)["\']'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, html)
            for match in matches:
                url = f"https://blog.naver.com/{blog_id}/{match}"
                if url not in urls:
                    urls.append(url)
                if len(urls) >= self.max_posts:
                    break

        return list(dict.fromkeys(urls))[:self.max_posts]

    async def _crawl_single_post(
        self,
        session: aiohttp.ClientSession,
        url: str,
        blog_id: str
    ) -> Optional[Dict]:
        """ë‹¨ì¼ ê²Œì‹œê¸€ í¬ë¡¤ë§"""
        # PC URL â†’ ëª¨ë°”ì¼ PostView URL ë³€í™˜
        match = re.search(rf'/{re.escape(blog_id)}/(\d+)', url)
        if not match:
            return None

        log_no = match.group(1)
        mobile_url = f"https://m.blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}"

        html = await self._fetch(session, mobile_url)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # ë°ì´í„° ì¶”ì¶œ
        data = {
            'url': url,
            'log_no': log_no,
            'title': self._extract_title(soup),
            'content': self._extract_content(soup),
            'images': self._extract_images(soup),
            'videos': self._extract_videos(soup),
            'hashtags': self._extract_hashtags(soup),
            'post_date': self._extract_date(soup),
            'view_count': self._extract_view_count(soup),
            'comment_count': self._extract_comment_count(soup),
            'sympathy_count': self._extract_sympathy_count(soup),
            'links': self._extract_links(soup),
            'category': self._extract_category(soup)
        }

        return data

    async def _get_blog_info(self, blog_id: str) -> Dict:
        """ë¸”ë¡œê·¸ ì •ë³´ ìˆ˜ì§‘"""
        async with aiohttp.ClientSession(headers=self.headers) as session:
            url = f"https://blog.naver.com/{blog_id}"
            html = await self._fetch(session, url)

            if not html:
                return {"blog_id": blog_id}

            soup = BeautifulSoup(html, 'html.parser')

            # ë¸”ë¡œê·¸ ì œëª©
            blog_title = "ì•Œ ìˆ˜ ì—†ìŒ"
            title_tag = soup.find('title')
            if title_tag:
                blog_title = title_tag.text.strip().split(':')[0].strip()

            return {
                "blog_id": blog_id,
                "blog_title": blog_title,
                "blog_url": url
            }

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        # 1. title íƒœê·¸
        title_tag = soup.find('title')
        if title_tag and title_tag.text:
            return title_tag.text.strip()

        # 2. og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()

        return "ì œëª© ì—†ìŒ"

    def _extract_content(self, soup: BeautifulSoup) -> Dict:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        # ì „ì²´ í…ìŠ¤íŠ¸
        text = soup.get_text(separator='\n', strip=True)

        # ë¬¸ë‹¨ë³„ ì¶”ì¶œ
        paragraphs = []
        for tag in soup.find_all(['p', 'div']):
            para_text = tag.get_text(strip=True)
            if para_text and len(para_text) > 10:
                paragraphs.append(para_text)

        return {
            "full_text": text,
            "length": len(text),
            "paragraphs": paragraphs,
            "paragraph_count": len(paragraphs)
        }

    def _extract_images(self, soup: BeautifulSoup) -> List[Dict]:
        """ì´ë¯¸ì§€ ì¶”ì¶œ"""
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                images.append({
                    "url": src,
                    "alt": img.get('alt', ''),
                    "width": img.get('width'),
                    "height": img.get('height')
                })
        return images

    def _extract_videos(self, soup: BeautifulSoup) -> List[Dict]:
        """ë™ì˜ìƒ ì¶”ì¶œ"""
        videos = []

        # video íƒœê·¸
        for video in soup.find_all('video'):
            src = video.get('src')
            if src:
                videos.append({"type": "video", "url": src})

        # iframe (ìœ íŠœë¸Œ ë“±)
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src')
            if src:
                videos.append({"type": "iframe", "url": src})

        return videos

    def _extract_hashtags(self, soup: BeautifulSoup) -> List[str]:
        """í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        hashtags = []

        # 1. __se-hash-tag
        for tag in soup.find_all('span', class_='__se-hash-tag'):
            text = tag.get_text(strip=True)
            if text:
                hashtags.append(text.replace('#', ''))

        # 2. í•˜ë‹¨ íƒœê·¸
        for tag in soup.select('div.wrap_tag a, a.link_tag'):
            text = tag.get_text(strip=True)
            if text:
                hashtags.append(text.replace('#', ''))

        # 3. ë©”íƒ€ íƒœê·¸
        for meta in soup.find_all('meta', attrs={'property': 'og:article:tag'}):
            content = meta.get('content')
            if content:
                hashtags.append(content.strip().replace('#', ''))

        # ì¤‘ë³µ ì œê±°
        return list(dict.fromkeys(hashtags))

    def _extract_date(self, soup: BeautifulSoup) -> Optional[str]:
        """í¬ìŠ¤íŒ… ë‚ ì§œ ì¶”ì¶œ"""
        # 1. ë©”íƒ€ íƒœê·¸
        date_meta = soup.find('meta', property='article:published_time')
        if date_meta and date_meta.get('content'):
            return date_meta['content'][:10]

        # 2. span.se_publishDate
        date_span = soup.find('span', class_='se_publishDate')
        if date_span:
            date_text = date_span.get_text(strip=True)
            match = re.search(r'(\d{4})[\.\-/](\d{1,2})[\.\-/](\d{1,2})', date_text)
            if match:
                year, month, day = match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

        # 3. time íƒœê·¸
        time_tag = soup.find('time')
        if time_tag and time_tag.get('datetime'):
            return time_tag['datetime'][:10]

        return None

    def _extract_view_count(self, soup: BeautifulSoup) -> Optional[int]:
        """ì¡°íšŒìˆ˜ ì¶”ì¶œ"""
        # 1. span.count
        count_span = soup.find('span', class_='count')
        if count_span:
            text = count_span.get_text(strip=True)
            match = re.search(r'[\d,]+', text)
            if match:
                return int(match.group().replace(',', ''))

        # 2. em.cnt
        cnt_em = soup.find('em', class_='cnt')
        if cnt_em:
            text = cnt_em.get_text(strip=True)
            match = re.search(r'[\d,]+', text)
            if match:
                return int(match.group().replace(',', ''))

        return None

    def _extract_comment_count(self, soup: BeautifulSoup) -> Optional[int]:
        """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ"""
        # 1. span.u_cbox_count
        comment_span = soup.find('span', class_='u_cbox_count')
        if comment_span:
            text = comment_span.get_text(strip=True)
            match = re.search(r'\d+', text)
            if match:
                return int(match.group())

        # 2. ëŒ“ê¸€ ê´€ë ¨ ë‹¤ë¥¸ ì…€ë ‰í„°
        for selector in ['.comment_count', '.cbox_count']:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                match = re.search(r'\d+', text)
                if match:
                    return int(match.group())

        return None

    def _extract_sympathy_count(self, soup: BeautifulSoup) -> Optional[int]:
        """ê³µê° ìˆ˜ ì¶”ì¶œ"""
        # 1. em.u_cnt
        sympathy_em = soup.find('em', class_='u_cnt')
        if sympathy_em:
            text = sympathy_em.get_text(strip=True)
            match = re.search(r'\d+', text)
            if match:
                return int(match.group())

        # 2. ê³µê° ê´€ë ¨ ë‹¤ë¥¸ ì…€ë ‰í„°
        for selector in ['.sympathy_count', '.cnt_sympathy']:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                match = re.search(r'\d+', text)
                if match:
                    return int(match.group())

        return None

    def _extract_links(self, soup: BeautifulSoup) -> Dict:
        """ë§í¬ ì¶”ì¶œ"""
        internal_links = []
        external_links = []

        for link in soup.find_all('a', href=True):
            href = link['href']

            # ë‚´ë¶€ ë§í¬ (ë„¤ì´ë²„ ë¸”ë¡œê·¸)
            if 'blog.naver.com' in href or href.startswith('/'):
                internal_links.append(href)
            else:
                external_links.append(href)

        return {
            "internal": internal_links,
            "external": external_links,
            "total_count": len(internal_links) + len(external_links)
        }

    def _extract_category(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        # 1. span.category
        category_span = soup.find('span', class_='category')
        if category_span:
            return category_span.get_text(strip=True)

        # 2. meta íƒœê·¸
        category_meta = soup.find('meta', property='article:section')
        if category_meta and category_meta.get('content'):
            return category_meta['content']

        return None


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì‚¬ìš© ì˜ˆì‹œ"""
    crawler = NaverBlogCrawler(max_posts=30)

    # ë¸”ë¡œê·¸ í¬ë¡¤ë§ (ì˜ˆ: coco_hodu_)
    blog_id = "coco_hodu_"
    result = await crawler.crawl_blog(blog_id, save_to_file=True)

    if result:
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"   - ë¸”ë¡œê·¸: {result['blog_info']['blog_title']}")
        print(f"   - ìˆ˜ì§‘ ê²Œì‹œê¸€: {result['total_posts']}ê°œ")
        print(f"   - í¬ë¡¤ë§ ì‹œê°„: {result['crawled_at']}")


if __name__ == "__main__":
    # ì‹¤í–‰
    asyncio.run(main())
