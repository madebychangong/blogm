#!/usr/bin/env python3
"""
ì¡°ì‚¬ ì²˜ë¦¬ ëª¨ë“ˆ

í•œê¸€ì ì¡°ì‚¬: ìš°íšŒ ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì • (AI í™œìš©)
ë‘ê¸€ì ì´ìƒ ì¡°ì‚¬: ë„ì–´ì“°ê¸° ì¶”ê°€
"""

import re
from typing import List, Tuple, Optional
import google.generativeai as genai
import os


class ParticleHandler:
    """í‚¤ì›Œë“œ ë’¤ì˜ ì¡°ì‚¬ ì²˜ë¦¬"""

    # í•œê¸€ì ì¡°ì‚¬ (ë„ì–´ì“°ê¸°í•˜ë©´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€ â†’ ìš°íšŒ í•„ìš”)
    SINGLE_PARTICLES = ['ë¥¼', 'ì„', 'ê°€', 'ì´', 'ëŠ”', 'ì€', 'ì—', 'ì˜', 'ë„', 'ë§Œ', 'ì™€', 'ê³¼']

    # ë‘ê¸€ì ì´ìƒ ì¡°ì‚¬ (ë„ì–´ì“°ê¸° OK)
    MULTI_PARTICLES = ['ìœ¼ë¡œ', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ë³´ë‹¤', 'ë§ˆì €', 'ì¡°ì°¨',
                       'ì´ë‚˜', 'ì´ë©°', 'ì´ë¼', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë§ˆë‹¤', 'ë¼ëŠ”', 'ì´ë€']

    def __init__(self, api_key: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            api_key: Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ì‚¬ìš©)
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')

        if self.api_key:
            # Gemini ì„¤ì •
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.5-pro')
        else:
            print("âš ï¸ Gemini API í‚¤ ì—†ìŒ - AI ìš°íšŒ ë¬¸ì¥ ìƒì„± ë¶ˆê°€")
            self.model = None

    def find_keyword_with_single_particle(self, text: str, keyword: str) -> List[Tuple[str, str]]:
        """
        í‚¤ì›Œë“œ+í•œê¸€ì ì¡°ì‚¬ ì°¾ê¸°

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            [(ì „ì²´ íŒ¨í„´, ì¡°ì‚¬), ...] ì˜ˆ: [("ê°•ë‚¨ ë§›ì§‘ì„", "ì„"), ...]
        """
        results = []

        for particle in self.SINGLE_PARTICLES:
            pattern = re.escape(keyword) + particle
            matches = re.finditer(pattern, text)

            for match in matches:
                results.append((match.group(), particle))

        return results

    def find_keyword_with_multi_particle(self, text: str, keyword: str) -> List[Tuple[str, str]]:
        """
        í‚¤ì›Œë“œ+ë‘ê¸€ì ì¡°ì‚¬ ì°¾ê¸° (ë„ì–´ì“°ê¸° ì—†ëŠ” ê²½ìš°)

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            [(ì „ì²´ íŒ¨í„´, ì¡°ì‚¬), ...] ì˜ˆ: [("ê°•ë‚¨ ë§›ì§‘ìœ¼ë¡œ", "ìœ¼ë¡œ"), ...]
        """
        results = []

        for particle in self.MULTI_PARTICLES:
            # ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì€ ê²½ìš°ë§Œ ì°¾ê¸°
            pattern = re.escape(keyword) + particle
            matches = re.finditer(pattern, text)

            for match in matches:
                # ì´ë¯¸ ë„ì–´ì“°ê¸°ê°€ ìˆëŠ”ì§€ í™•ì¸ (í‚¤ì›Œë“œ ê³µë°± ì¡°ì‚¬)
                before_match = match.start() - 1
                if before_match >= 0 and text[before_match] == ' ':
                    # ì´ë¯¸ ë„ì–´ì“°ê¸° ìˆìŒ - ìŠ¤í‚µ
                    continue

                results.append((match.group(), particle))

        return results

    def create_workaround_prompt(self, sentence: str, keyword: str, particle: str) -> str:
        """
        ìš°íšŒ ë¬¸ì¥ ìƒì„± í”„ë¡¬í”„íŠ¸

        Args:
            sentence: ì›ë³¸ ë¬¸ì¥
            keyword: í†µ í‚¤ì›Œë“œ
            particle: ì¡°ì‚¬

        Returns:
            í”„ë¡¬í”„íŠ¸
        """
        prompt = f"""ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìˆ˜ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ìš”ì²­

ì•„ë˜ ë¬¸ì¥ì—ì„œ "{keyword}{particle}"ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.

**ì œì•½ ì¡°ê±´:**
1. "{keyword}" ë’¤ì— í•œê¸€ì ì¡°ì‚¬ "{particle}"ê°€ ë°”ë¡œ ë¶™ìœ¼ë©´ SEOì—ì„œ ì¹´ìš´íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
2. "{keyword} {particle}" ì²˜ëŸ¼ ë„ì–´ì“°ê¸°í•˜ë©´ ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤
3. ë”°ë¼ì„œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë°”ê¿”ì„œ "{keyword}"ê°€ ì¹´ìš´íŠ¸ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤

**ìˆ˜ì • ë°©ë²•:**
- "{keyword}{particle}" ë¶€ë¶„ì„ "{keyword} [ì¶”ê°€ ë‹¨ì–´]"ë¡œ ìˆ˜ì •
- ì¡°ì‚¬ëŠ” ì¶”ê°€ ë‹¨ì–´ ë’¤ì— ë¶™ì´ê¸°
- ë¬¸ì¥ì˜ ì˜ë¯¸ëŠ” ìµœëŒ€í•œ ìœ ì§€
- ìì—°ìŠ¤ëŸ¬ìš´ ë¸”ë¡œê·¸ ë§íˆ¬ ìœ ì§€

**ì˜ˆì‹œ:**
```
ì…ë ¥: "ê°•ë‚¨ ë§›ì§‘ì„ ì°¾ê³  ìˆì–´ìš”"
ì¶œë ¥: "ê°•ë‚¨ ë§›ì§‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ê³  ìˆì–´ìš”"
â†’ "ê°•ë‚¨ ë§›ì§‘" ì¹´ìš´íŠ¸ ê°€ëŠ¥! âœ…

ì…ë ¥: "ê°±ë…„ê¸° í™ì¡°ê°€ ì‹¬í•´ìš”"
ì¶œë ¥: "ê°±ë…„ê¸° í™ì¡° ì¦ì„¸ê°€ ì‹¬í•´ìš”"
â†’ "ê°±ë…„ê¸° í™ì¡°" ì¹´ìš´íŠ¸ ê°€ëŠ¥! âœ…

ì…ë ¥: "í”¼ë¶€ê³¼ë¥¼ ë°©ë¬¸í–ˆì–´ìš”"
ì¶œë ¥: "í”¼ë¶€ê³¼ ë³‘ì›ì„ ë°©ë¬¸í–ˆì–´ìš”"
â†’ "í”¼ë¶€ê³¼" ì¹´ìš´íŠ¸ ê°€ëŠ¥! âœ…
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ì›ë³¸ ë¬¸ì¥
{sentence}

# ìˆ˜ì •í•  ë¶€ë¶„
"{keyword}{particle}" â†’ "{keyword} [ì¶”ê°€ ë‹¨ì–´]"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ì¶œë ¥
ìˆ˜ì •ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´.
"""
        return prompt

    def fix_single_particle_with_ai(self, text: str, keyword: str) -> str:
        """
        í•œê¸€ì ì¡°ì‚¬ â†’ AIë¡œ ìš°íšŒ ë¬¸ì¥ ìƒì„±

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        if not self.model:
            print("âš ï¸ Gemini API í‚¤ ì—†ìŒ - í•œê¸€ì ì¡°ì‚¬ ì²˜ë¦¬ ë¶ˆê°€")
            return text

        # í‚¤ì›Œë“œ+í•œê¸€ì ì¡°ì‚¬ ì°¾ê¸°
        patterns = self.find_keyword_with_single_particle(text, keyword)

        if not patterns:
            return text

        print(f"\nğŸ”§ í•œê¸€ì ì¡°ì‚¬ ë°œê²¬: {len(patterns)}ê°œ")

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'([.!?])', text)
        # êµ¬ë¶„ì í¬í•¨í•´ì„œ ë‹¤ì‹œ í•©ì¹˜ê¸°
        sentences = [''.join(sentences[i:i+2]) for i in range(0, len(sentences)-1, 2)]
        if len(text) > 0 and text[-1] not in '.!?':
            # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë§ˆì¹¨í‘œ ì—†ìœ¼ë©´ ì¶”ê°€
            sentences.append(''.join(sentences[-2:]) if len(sentences) >= 2 else text)

        modified_text = text

        for pattern, particle in patterns:
            # ì´ íŒ¨í„´ì´ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            for sentence in sentences:
                if pattern in sentence:
                    print(f"  - ìˆ˜ì • ì¤‘: '{pattern}' in '{sentence.strip()}'")

                    # AIë¡œ ìš°íšŒ ë¬¸ì¥ ìƒì„±
                    try:
                        prompt = self.create_workaround_prompt(sentence, keyword, particle)
                        response = self.model.generate_content(prompt)

                        if response.text:
                            new_sentence = response.text.strip()
                            modified_text = modified_text.replace(sentence, new_sentence)
                            print(f"    â†’ '{new_sentence.strip()}'")
                            break

                    except Exception as e:
                        print(f"    âš ï¸ AI ì˜¤ë¥˜: {e}")
                        continue

        return modified_text

    def fix_multi_particle(self, text: str, keyword: str) -> str:
        """
        ë‘ê¸€ì ì¡°ì‚¬ â†’ ë„ì–´ì“°ê¸° ì¶”ê°€

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        # í‚¤ì›Œë“œ+ë‘ê¸€ì ì¡°ì‚¬ ì°¾ê¸° (ë„ì–´ì“°ê¸° ì—†ëŠ” ê²½ìš°)
        patterns = self.find_keyword_with_multi_particle(text, keyword)

        if not patterns:
            return text

        print(f"\nâœï¸ ë‘ê¸€ì ì¡°ì‚¬ ë°œê²¬: {len(patterns)}ê°œ - ë„ì–´ì“°ê¸° ì¶”ê°€")

        modified_text = text

        for pattern, particle in patterns:
            # "í‚¤ì›Œë“œì¡°ì‚¬" â†’ "í‚¤ì›Œë“œ ì¡°ì‚¬"
            new_pattern = keyword + ' ' + particle
            modified_text = modified_text.replace(pattern, new_pattern)
            print(f"  - '{pattern}' â†’ '{new_pattern}'")

        return modified_text

    def fix_all_particles(self, text: str, keyword: str) -> str:
        """
        ëª¨ë“  ì¡°ì‚¬ ì²˜ë¦¬ (í•œê¸€ì + ë‘ê¸€ì)

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        print(f"\n{'='*80}")
        print(f"ì¡°ì‚¬ ì²˜ë¦¬: {keyword}")
        print(f"{'='*80}")

        # 1. ë‘ê¸€ì ì¡°ì‚¬ ë¨¼ì € ì²˜ë¦¬ (ê°„ë‹¨)
        text = self.fix_multi_particle(text, keyword)

        # 2. í•œê¸€ì ì¡°ì‚¬ ì²˜ë¦¬ (AI í•„ìš”)
        text = self.fix_single_particle_with_ai(text, keyword)

        print(f"{'='*80}\n")

        return text


def test_particle_handler():
    """í…ŒìŠ¤íŠ¸"""

    # API í‚¤ í™•ì¸
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("âŒ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    handler = ParticleHandler()

    # í…ŒìŠ¤íŠ¸ 1: ë‘ê¸€ì ì¡°ì‚¬ ë„ì–´ì“°ê¸°
    print("=" * 80)
    print("í…ŒìŠ¤íŠ¸ 1: ë‘ê¸€ì ì¡°ì‚¬ ë„ì–´ì“°ê¸°")
    print("=" * 80)

    test_text1 = "ê°•ë‚¨ ë§›ì§‘ìœ¼ë¡œ ë¦¬ê¼¬ë¥´ë¼ëŠ” ë ˆìŠ¤í† ë‘ì„ ë‹¤ë…€ì™”ì–´ìš”."
    keyword1 = "ê°•ë‚¨ ë§›ì§‘"

    print(f"\nì›ë³¸: {test_text1}")

    result1 = handler.fix_multi_particle(test_text1, keyword1)

    print(f"\nìˆ˜ì •: {result1}")
    print(f"ê¸°ëŒ€: ê°•ë‚¨ ë§›ì§‘ ìœ¼ë¡œ ë¦¬ê¼¬ë¥´ë¼ëŠ” ë ˆìŠ¤í† ë‘ì„ ë‹¤ë…€ì™”ì–´ìš”.")

    # í…ŒìŠ¤íŠ¸ 2: í•œê¸€ì ì¡°ì‚¬ ìš°íšŒ
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 2: í•œê¸€ì ì¡°ì‚¬ ìš°íšŒ (AI)")
    print("=" * 80)

    test_text2 = "ê°•ë‚¨ ë§›ì§‘ì„ ì°¾ê³  ìˆì–´ìš”. ê°•ë‚¨ ë§›ì§‘ê°€ ë§ë‹¤ê³  ë“¤ì—ˆì–´ìš”."
    keyword2 = "ê°•ë‚¨ ë§›ì§‘"

    print(f"\nì›ë³¸: {test_text2}")

    result2 = handler.fix_single_particle_with_ai(test_text2, keyword2)

    print(f"\nìˆ˜ì •: {result2}")

    # í…ŒìŠ¤íŠ¸ 3: ì „ì²´ ì²˜ë¦¬
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 3: ì „ì²´ ì²˜ë¦¬")
    print("=" * 80)

    test_text3 = """ê°•ë‚¨ ë§›ì§‘ì„ ì°¾ê³  ìˆì–´ìš”.
ê°•ë‚¨ ë§›ì§‘ìœ¼ë¡œ ì¶”ì²œ ë°›ê³  ì‹¶ì–´ìš”.
ê°•ë‚¨ ë§›ì§‘ê°€ ë§ë‹¤ê³  í•´ì„œ ê¶ê¸ˆí•´ìš”."""

    keyword3 = "ê°•ë‚¨ ë§›ì§‘"

    print(f"\nì›ë³¸:\n{test_text3}")

    result3 = handler.fix_all_particles(test_text3, keyword3)

    print(f"\nìµœì¢… ê²°ê³¼:\n{result3}")


if __name__ == '__main__':
    test_particle_handler()
