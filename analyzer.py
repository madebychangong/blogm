"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¶„ì„ê¸° - ì›¹ ë²„ì „
2025 ë„¤ì´ë²„ C-Rank ê¸°ì¤€
- ë‹¤ì¤‘ í´ë°± URL ìˆ˜ì§‘ (PC â†’ iframe â†’ ëª¨ë°”ì¼ â†’ RSS)
- ëª¨ë°”ì¼ PostViewë¡œ ì•ˆì •ì  íŒŒì‹±
- ì„¸ë¶„í™”ëœ ì ìˆ˜ ì²´ê³„ (SEO + ì½˜í…ì¸ )
- ë“±ê¸‰ì œ (S~F)
- ë¹„ë™ê¸° HTTP ìš”ì²­ìœ¼ë¡œ ë¹ ë¥¸ í¬ë¡¤ë§
- í¬ìŠ¤íŒ… ë‚ ì§œ, ì¡°íšŒìˆ˜ ì¶”ì¶œ
- ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  APIë¡œ í‚¤ì›Œë“œ ê²½ìŸë ¥ ë¶„ì„
"""
import re
import requests
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from collections import Counter
from datetime import datetime
from typing import Optional, List, Dict

# ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API (optional)
try:
    from naver_ad_api import NaverAdAPI
    NAVER_AD_API_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API ì‚¬ìš© ë¶ˆê°€: {e}")
    NAVER_AD_API_AVAILABLE = False

class BlogAnalyzer:
    def __init__(self, enable_keyword_analysis: bool = True):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36',
            'Referer': 'https://blog.naver.com/',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        self.timeout = 12
        self.max_posts = 30  # ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜ ì¦ê°€

        # ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API ì´ˆê¸°í™”
        self.keyword_api = None
        self.enable_keyword_analysis = enable_keyword_analysis and NAVER_AD_API_AVAILABLE

        if self.enable_keyword_analysis:
            try:
                self.keyword_api = NaverAdAPI()
                print("âœ… í‚¤ì›Œë“œ ê²½ìŸë ¥ ë¶„ì„ í™œì„±í™”")
            except Exception as e:
                print(f"âš ï¸ í‚¤ì›Œë“œ API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enable_keyword_analysis = False
    
    def analyze(self, blog_id):
        """ë¸”ë¡œê·¸ ì „ì²´ ë¶„ì„ (ë™ê¸° ë˜í¼)"""
        try:
            # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            return asyncio.run(self._analyze_async(blog_id))
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None

    async def _analyze_async(self, blog_id):
        """ë¸”ë¡œê·¸ ì „ì²´ ë¶„ì„ (ë¹„ë™ê¸°)"""
        try:
            # 1. ê²Œì‹œê¸€ URL ìˆ˜ì§‘ (ë‹¤ì¤‘ í´ë°±)
            post_urls = await self._get_recent_post_urls_async(blog_id)
            if not post_urls:
                return None

            # 2. ê° ê²Œì‹œê¸€ ë³‘ë ¬ ë¶„ì„ (ìµœëŒ€ 30ê°œ)
            post_results = []
            async with aiohttp.ClientSession(headers=self.headers) as session:
                tasks = [
                    self._analyze_single_post_async(session, url, blog_id)
                    for url in post_urls[:self.max_posts]
                ]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì¶”ê°€
                for result in results:
                    if result and not isinstance(result, Exception):
                        post_results.append(result)

            if not post_results:
                return None

            # 3. ë¸”ë¡œê·¸ ì „ì²´ ë­í¬ ê³„ì‚°
            blog_rank, traffic_rank = self._calculate_blog_rank(post_results)

            # 4. í•´ì‹œíƒœê·¸ ê²½ìŸë ¥ ë¶„ì„ (optional)
            keyword_analysis = None
            if self.enable_keyword_analysis and self.keyword_api:
                keyword_analysis = await self._analyze_hashtag_competition(post_results)

            result = {
                "blog_id": blog_id,
                "total_posts": len(post_results),
                "posts": post_results,
                "blog_rank": blog_rank,
                "traffic_rank": traffic_rank,
                "analyzed_at": datetime.now().isoformat()
            }

            # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if keyword_analysis:
                result["keyword_analysis"] = keyword_analysis

            return result

        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def _fetch(self, url, timeout=None):
        """HTTP ìš”ì²­ (ë™ê¸°)"""
        res = requests.get(url, headers=self.headers, timeout=timeout or self.timeout)
        if not res.encoding or res.encoding.lower() in ("iso-8859-1", "ansi"):
            res.encoding = res.apparent_encoding
        res.raise_for_status()
        return res

    async def _fetch_async(self, session, url, timeout=None):
        """HTTP ìš”ì²­ (ë¹„ë™ê¸°)"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout or self.timeout)) as response:
                response.raise_for_status()
                text = await response.text()
                return text
        except Exception as e:
            print(f"HTTP ìš”ì²­ ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def _get_recent_post_urls(self, blog_id):
        """ê²Œì‹œê¸€ URL ìˆ˜ì§‘ (ë‹¤ì¤‘ í´ë°±)"""
        urls = []
        
        # ë°©ë²• 1: PC ë©”ì¸ â†’ iframe
        try:
            main_url = f"https://blog.naver.com/{blog_id}"
            res = self._fetch(main_url)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # iframe ì°¾ê¸°
            iframe = soup.find('iframe', id='mainFrame')
            if iframe and iframe.get('src'):
                inner_url = iframe['src']
                if inner_url.startswith('/'):
                    inner_url = 'https://blog.naver.com' + inner_url
                
                res2 = self._fetch(inner_url)
                urls = self._extract_urls_from_html(res2.text, blog_id)
                
                if urls:
                    return urls[:10]
        except:
            pass
        
        # ë°©ë²• 2: PostList (êµ¬í˜•)
        try:
            list_url = f"https://blog.naver.com/PostList.naver?blogId={blog_id}&widgetTypeCall=true&directAccess=true"
            res = self._fetch(list_url)
            urls = self._extract_urls_from_html(res.text, blog_id)
            
            if urls:
                return urls[:10]
        except:
            pass
        
        # ë°©ë²• 3: ëª¨ë°”ì¼ í™ˆ
        try:
            mobile_url = f"https://m.blog.naver.com/{blog_id}"
            res = self._fetch(mobile_url)
            urls = self._extract_urls_from_html(res.text, blog_id)
            
            if urls:
                return urls[:10]
        except:
            pass
        
        # ë°©ë²• 4: RSS
        try:
            rss_url = f"https://rss.blog.naver.com/{blog_id}.xml"
            res = self._fetch(rss_url)
            soup = BeautifulSoup(res.text, 'xml')
            
            urls = []
            for item in soup.find_all('item'):
                link = item.find('link')
                if link and link.text:
                    log_nos = re.findall(r'logNo=(\d+)', link.text)
                    if not log_nos:
                        log_nos = re.findall(rf'/{re.escape(blog_id)}/(\d+)', link.text)
                    if log_nos:
                        urls.append(f"https://blog.naver.com/{blog_id}/{log_nos[0]}")
                if len(urls) >= 10:
                    break
            
            if urls:
                return urls[:10]
        except:
            pass
        
        return []

    async def _get_recent_post_urls_async(self, blog_id):
        """ê²Œì‹œê¸€ URL ìˆ˜ì§‘ (ë¹„ë™ê¸°, ë‹¤ì¤‘ í´ë°±)"""
        async with aiohttp.ClientSession(headers=self.headers) as session:
            urls = []

            # ë°©ë²• 1: PC ë©”ì¸ â†’ iframe
            try:
                main_url = f"https://blog.naver.com/{blog_id}"
                html = await self._fetch_async(session, main_url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    iframe = soup.find('iframe', id='mainFrame')
                    if iframe and iframe.get('src'):
                        inner_url = iframe['src']
                        if inner_url.startswith('/'):
                            inner_url = 'https://blog.naver.com' + inner_url

                        inner_html = await self._fetch_async(session, inner_url)
                        if inner_html:
                            urls = self._extract_urls_from_html(inner_html, blog_id)
                            if urls:
                                return urls[:self.max_posts]
            except Exception as e:
                print(f"ë°©ë²• 1 ì‹¤íŒ¨: {e}")

            # ë°©ë²• 2: PostList (êµ¬í˜•)
            try:
                list_url = f"https://blog.naver.com/PostList.naver?blogId={blog_id}&widgetTypeCall=true&directAccess=true"
                html = await self._fetch_async(session, list_url)
                if html:
                    urls = self._extract_urls_from_html(html, blog_id)
                    if urls:
                        return urls[:self.max_posts]
            except Exception as e:
                print(f"ë°©ë²• 2 ì‹¤íŒ¨: {e}")

            # ë°©ë²• 3: ëª¨ë°”ì¼ í™ˆ
            try:
                mobile_url = f"https://m.blog.naver.com/{blog_id}"
                html = await self._fetch_async(session, mobile_url)
                if html:
                    urls = self._extract_urls_from_html(html, blog_id)
                    if urls:
                        return urls[:self.max_posts]
            except Exception as e:
                print(f"ë°©ë²• 3 ì‹¤íŒ¨: {e}")

            # ë°©ë²• 4: RSS
            try:
                rss_url = f"https://rss.blog.naver.com/{blog_id}.xml"
                html = await self._fetch_async(session, rss_url)
                if html:
                    soup = BeautifulSoup(html, 'xml')
                    urls = []
                    for item in soup.find_all('item'):
                        link = item.find('link')
                        if link and link.text:
                            log_nos = re.findall(r'logNo=(\d+)', link.text)
                            if not log_nos:
                                log_nos = re.findall(rf'/{re.escape(blog_id)}/(\d+)', link.text)
                            if log_nos:
                                urls.append(f"https://blog.naver.com/{blog_id}/{log_nos[0]}")
                        if len(urls) >= self.max_posts:
                            break

                    if urls:
                        return urls[:self.max_posts]
            except Exception as e:
                print(f"ë°©ë²• 4 ì‹¤íŒ¨: {e}")

            return []

    def _extract_urls_from_html(self, html, blog_id):
        """HTMLì—ì„œ ê²Œì‹œê¸€ URL ì¶”ì¶œ"""
        urls = []
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
        patterns = [
            rf'/{re.escape(blog_id)}/(\d+)',
            r'logNo=(\d+)',
            r'data-log-no=["\'](\d+)["\']'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html)
            for match in matches:
                url = f"https://blog.naver.com/{blog_id}/{match}"
                if url not in urls:
                    urls.append(url)
                if len(urls) >= 10:
                    break
        
        return list(dict.fromkeys(urls))[:10]  # ì¤‘ë³µ ì œê±°
    
    def _to_mobile_postview(self, url, blog_id):
        """PC URL â†’ ëª¨ë°”ì¼ PostView URL ë³€í™˜"""
        match = re.search(rf'https?://blog\.naver\.com/{re.escape(blog_id)}/(\d+)', url)
        if match:
            return f"https://m.blog.naver.com/PostView.naver?blogId={blog_id}&logNo={match.group(1)}"
        
        match = re.search(r'logNo=(\d+)', url)
        if match:
            return f"https://m.blog.naver.com/PostView.naver?blogId={blog_id}&logNo={match.group(1)}"
        
        return None
    
    def _analyze_single_post(self, url, blog_id):
        """ê°œë³„ ê²Œì‹œê¸€ ë¶„ì„"""
        # ëª¨ë°”ì¼ PostViewë¡œ ë³€í™˜
        mobile_url = self._to_mobile_postview(url, blog_id)
        if not mobile_url:
            return None
        
        try:
            res = self._fetch(mobile_url)
            soup = BeautifulSoup(res.text, 'html.parser')
        except:
            return None
        
        # ë°ì´í„° ì¶”ì¶œ
        post_data = self._extract_post_data(soup, url)
        
        # ì ìˆ˜ ê³„ì‚°
        seo_score, seo_issues = self._calculate_seo_score(post_data)
        content_score, content_issues = self._calculate_content_score(post_data)
        
        total_score = int(seo_score * 0.4 + content_score * 0.6)
        
        return {
            'title': post_data['title'][:50] + '...' if len(post_data['title']) > 50 else post_data['title'],
            'url': url,
            'total_score': total_score,
            'seo_score': seo_score,
            'content_score': content_score,
            'text_length': post_data['text_length'],
            'image_count': post_data['image_count'],
            'video_count': post_data['video_count'],
            'hashtag_count': len(post_data['hashtags']),
            'link_count': post_data['link_count'],
            'issues': seo_issues + content_issues
        }

    async def _analyze_single_post_async(self, session, url, blog_id):
        """ê°œë³„ ê²Œì‹œê¸€ ë¶„ì„ (ë¹„ë™ê¸°)"""
        # ëª¨ë°”ì¼ PostViewë¡œ ë³€í™˜
        mobile_url = self._to_mobile_postview(url, blog_id)
        if not mobile_url:
            return None

        try:
            html = await self._fetch_async(session, mobile_url)
            if not html:
                return None

            soup = BeautifulSoup(html, 'html.parser')
        except Exception as e:
            print(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")
            return None

        # ë°ì´í„° ì¶”ì¶œ
        post_data = self._extract_post_data(soup, url)

        # ì ìˆ˜ ê³„ì‚°
        seo_score, seo_issues = self._calculate_seo_score(post_data)
        content_score, content_issues = self._calculate_content_score(post_data)

        total_score = int(seo_score * 0.4 + content_score * 0.6)

        result = {
            'title': post_data['title'][:50] + '...' if len(post_data['title']) > 50 else post_data['title'],
            'url': url,
            'total_score': total_score,
            'seo_score': seo_score,
            'content_score': content_score,
            'text_length': post_data['text_length'],
            'image_count': post_data['image_count'],
            'video_count': post_data['video_count'],
            'hashtag_count': len(post_data['hashtags']),
            'link_count': post_data['link_count'],
            'issues': seo_issues + content_issues
        }

        # í¬ìŠ¤íŒ… ë‚ ì§œ ì¶”ê°€
        if post_data.get('post_date'):
            result['post_date'] = post_data['post_date']

        # ì¡°íšŒìˆ˜ ì¶”ê°€
        if post_data.get('view_count'):
            result['view_count'] = post_data['view_count']

        return result

    def _extract_post_data(self, soup, url):
        """ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ"""
        # ì œëª©
        title = 'ì œëª© ì—†ìŒ'
        title_tag = soup.find('title')
        if title_tag and title_tag.text:
            title = title_tag.text.strip()
        else:
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                title = og_title['content'].strip()
        
        # í•´ì‹œíƒœê·¸ (ë‹¤ì¤‘ ì†ŒìŠ¤)
        hashtags = []
        # 1) ë³¸ë¬¸ __se-hash-tag
        hashtags.extend(t.get_text(strip=True) for t in soup.find_all('span', class_='__se-hash-tag'))
        # 2) í•˜ë‹¨ íƒœê·¸
        hashtags.extend(a.get_text(strip=True) for a in soup.select('div.wrap_tag a'))
        hashtags.extend(a.get_text(strip=True) for a in soup.select('a.link_tag'))
        # 3) ë©”íƒ€ íƒœê·¸
        for meta in soup.find_all('meta', attrs={'property': 'og:article:tag'}):
            if meta.get('content'):
                hashtags.append(meta['content'].strip())
        
        # í•´ì‹œíƒœê·¸ ì •ê·œí™”
        normalized_tags = []
        seen = set()
        for tag in hashtags:
            if not tag:
                continue
            tag = tag if tag.startswith('#') else ('#' + tag)
            if tag not in seen:
                normalized_tags.append(tag[1:])  # # ì œê±°
                seen.add(tag)
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸
        content_text = soup.get_text(separator=' ', strip=True)
        
        # ë¬¸ë‹¨ ì¶”ì¶œ
        paragraphs = []
        for p_tag in soup.find_all(['p', 'div']):
            text = p_tag.get_text(strip=True)
            if text and len(text) > 10:
                paragraphs.append(text)
        
        # ì´ë¯¸ì§€
        images = soup.find_all('img')
        
        # ë™ì˜ìƒ
        videos = soup.find_all(['video', 'iframe'])
        
        # ë§í¬
        links = soup.find_all('a', href=True)

        # í¬ìŠ¤íŒ… ë‚ ì§œ ì¶”ì¶œ (ë‹¤ì¤‘ ì†ŒìŠ¤)
        post_date = None
        try:
            # 1. ë©”íƒ€ íƒœê·¸
            date_meta = soup.find('meta', property='article:published_time')
            if date_meta and date_meta.get('content'):
                post_date = date_meta['content'][:10]  # YYYY-MM-DD í˜•ì‹

            # 2. span.se_publishDate
            if not post_date:
                date_span = soup.find('span', class_='se_publishDate')
                if date_span:
                    date_text = date_span.get_text(strip=True)
                    # ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "2025.01.15." â†’ "2025-01-15")
                    date_match = re.search(r'(\d{4})[\.\-/](\d{1,2})[\.\-/](\d{1,2})', date_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        post_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"

            # 3. time íƒœê·¸
            if not post_date:
                time_tag = soup.find('time')
                if time_tag and time_tag.get('datetime'):
                    post_date = time_tag['datetime'][:10]
        except Exception as e:
            print(f"ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        view_count = None
        try:
            # 1. span.count
            count_span = soup.find('span', class_='count')
            if count_span:
                view_text = count_span.get_text(strip=True)
                # "ì¡°íšŒ 1,234" â†’ 1234
                view_match = re.search(r'[\d,]+', view_text)
                if view_match:
                    view_count = int(view_match.group().replace(',', ''))

            # 2. em.cnt
            if view_count is None:
                cnt_em = soup.find('em', class_='cnt')
                if cnt_em:
                    view_text = cnt_em.get_text(strip=True)
                    view_match = re.search(r'[\d,]+', view_text)
                    if view_match:
                        view_count = int(view_match.group().replace(',', ''))
        except Exception as e:
            print(f"ì¡°íšŒìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        result = {
            'title': title,
            'url': url,
            'text': content_text,
            'text_length': len(content_text),
            'paragraphs': paragraphs,
            'paragraph_count': len(paragraphs),
            'first_paragraph': paragraphs[0] if paragraphs else '',
            'hashtags': normalized_tags,
            'image_count': len(images),
            'video_count': len(videos),
            'link_count': len(links)
        }

        # ì„ íƒì  í•„ë“œ ì¶”ê°€
        if post_date:
            result['post_date'] = post_date
        if view_count is not None:
            result['view_count'] = view_count

        return result
    
    def _extract_keywords(self, title, top_n=3):
        """ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        title = re.sub(r'[^\w\s]', ' ', title)
        words = [w for w in title.split() if len(w) >= 2]
        
        stopwords = ['ì´ë²ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°',
                     'ìˆëŠ”', 'ì—†ëŠ”', 'ë˜ëŠ”', 'í•˜ëŠ”', 'ì´ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ì •ë§', 'ì§„ì§œ',
                     'ë„ˆë¬´', 'ì•„ì£¼', 'ë§¤ìš°', 'ì™„ì „', 'í›„ê¸°', 'ë¦¬ë·°', 'ì¶”ì²œ']
        words = [w for w in words if w not in stopwords]
        
        if not words:
            return []
        
        word_freq = Counter(words)
        return [word for word, count in word_freq.most_common(top_n)]
    
    def _calculate_seo_score(self, post_data):
        """SEO ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì , ê°ì ì‹)"""
        score = 100
        issues = []
        
        title = post_data['title']
        hashtags = post_data['hashtags']
        text = post_data['text']
        first_paragraph = post_data['first_paragraph']
        link_count = post_data['link_count']
        
        # 1. ì œëª© ê¸¸ì´ (15~40ì ê¶Œì¥)
        title_len = len(title)
        if title_len < 15:
            score -= 10
            issues.append(f"ì œëª©ì´ ì§§ìŒ ({title_len}ì)")
        elif title_len > 40:
            score -= 5
            issues.append(f"ì œëª©ì´ ê¹€ ({title_len}ì)")
        
        # 2. í•´ì‹œíƒœê·¸ (8~10ê°œ ìµœì )
        tag_count = len(hashtags)
        if 8 <= tag_count <= 10:
            pass
        elif 5 <= tag_count < 8:
            score -= 5
            issues.append(f"í•´ì‹œíƒœê·¸ ë¶€ì¡± ({tag_count}ê°œ)")
        elif 3 <= tag_count < 5:
            score -= 10
            issues.append(f"í•´ì‹œíƒœê·¸ ë§ì´ ë¶€ì¡± ({tag_count}ê°œ)")
        elif tag_count == 0:
            score -= 20
            issues.append("í•´ì‹œíƒœê·¸ ì—†ìŒ")
        elif tag_count < 3:
            score -= 15
            issues.append(f"í•´ì‹œíƒœê·¸ ë§¤ìš° ë¶€ì¡± ({tag_count}ê°œ)")
        elif tag_count > 15:
            score -= 8
            issues.append(f"í•´ì‹œíƒœê·¸ ê³¼ë‹¤ ({tag_count}ê°œ)")
        
        # 3. í‚¤ì›Œë“œ ë¶„ì„
        keywords = self._extract_keywords(title)
        if keywords:
            # í‚¤ì›Œë“œ-í•´ì‹œíƒœê·¸ ì¼ì¹˜
            keyword_in_hashtag = sum(1 for kw in keywords if kw in hashtags)
            if keyword_in_hashtag == 0:
                score -= 10
                issues.append("í‚¤ì›Œë“œì™€ í•´ì‹œíƒœê·¸ ë¶ˆì¼ì¹˜")
            
            # ë„ì…ë¶€ì— í‚¤ì›Œë“œ
            first_200 = first_paragraph[:200] if len(first_paragraph) > 200 else first_paragraph
            keyword_in_first = sum(1 for kw in keywords if kw in first_200)
            if keyword_in_first == 0 and len(first_paragraph) > 0:
                score -= 8
                issues.append("ë„ì…ë¶€ì— í‚¤ì›Œë“œ ì—†ìŒ")
            
            # í‚¤ì›Œë“œ ë°€ë„
            if len(text) > 0:
                keyword_count = sum(text.count(kw) for kw in keywords)
                if keyword_count < 3:
                    score -= 7
                    issues.append(f"í‚¤ì›Œë“œ ë°˜ë³µ ë¶€ì¡± ({keyword_count}íšŒ)")
                elif keyword_count > 10:
                    score -= 5
                    issues.append(f"í‚¤ì›Œë“œ ê³¼ë‹¤ ({keyword_count}íšŒ)")
        
        # 4. ë§í¬
        if link_count == 0:
            score -= 5
            issues.append("ë§í¬ ì—†ìŒ")
        elif link_count > 10:
            score -= 3
            issues.append(f"ë§í¬ ê³¼ë‹¤ ({link_count}ê°œ)")
        
        return max(0, score), issues
    
    def _calculate_content_score(self, post_data):
        """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ (100ì  ë§Œì )"""
        score = 0
        issues = []
        
        text_length = post_data['text_length']
        image_count = post_data['image_count']
        video_count = post_data['video_count']
        paragraph_count = post_data['paragraph_count']
        paragraphs = post_data['paragraphs']
        
        # 1. ê¸€ììˆ˜ (45ì )
        if text_length >= 3000:
            score += 45
        elif text_length >= 2500:
            score += 40
        elif text_length >= 2000:
            score += 35
        elif text_length >= 1500:
            score += 25
            issues.append(f"ê¸€ììˆ˜ ë¶€ì¡± ({text_length}ì)")
        elif text_length >= 1000:
            score += 15
            issues.append(f"ê¸€ììˆ˜ ë§ì´ ë¶€ì¡± ({text_length}ì)")
        elif text_length >= 500:
            score += 5
            issues.append(f"ê¸€ììˆ˜ ë§¤ìš° ë¶€ì¡± ({text_length}ì)")
        else:
            issues.append(f"ê¸€ììˆ˜ ì‹¬ê°í•˜ê²Œ ë¶€ì¡± ({text_length}ì)")
        
        # 2. ì´ë¯¸ì§€ (35ì )
        if image_count >= 10:
            score += 35
        elif image_count >= 7:
            score += 30
        elif image_count >= 5:
            score += 25
            issues.append(f"ì´ë¯¸ì§€ ë¶€ì¡± ({image_count}ì¥)")
        elif image_count >= 3:
            score += 15
            issues.append(f"ì´ë¯¸ì§€ ë§ì´ ë¶€ì¡± ({image_count}ì¥)")
        elif image_count >= 1:
            score += 5
            issues.append(f"ì´ë¯¸ì§€ ë§¤ìš° ë¶€ì¡± ({image_count}ì¥)")
        else:
            issues.append("ì´ë¯¸ì§€ ì—†ìŒ")
        
        # 3. ë™ì˜ìƒ (10ì )
        if video_count >= 1:
            score += 10
        
        # 4. ë¬¸ë‹¨ êµ¬ì¡° (10ì )
        if paragraph_count >= 8:
            score += 10
        elif paragraph_count >= 5:
            score += 7
        elif paragraph_count >= 3:
            score += 5
            issues.append(f"ë¬¸ë‹¨ ë¶€ì¡± ({paragraph_count}ê°œ)")
        elif paragraph_count >= 1:
            score += 3
            issues.append(f"ë¬¸ë‹¨ ë§¤ìš° ë¶€ì¡± ({paragraph_count}ê°œ)")
        else:
            issues.append("ë¬¸ë‹¨ êµ¬ì¡° ì—†ìŒ")
        
        # ë¬¸ë‹¨ ê¸¸ì´ ì²´í¬
        if paragraphs:
            long_paragraphs = sum(1 for p in paragraphs if len(p) > 300)
            if long_paragraphs > paragraph_count * 0.4:
                score -= 3
                issues.append("ì¼ë¶€ ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¹€")
        
        return score, issues
    
    def _calculate_blog_rank(self, post_results):
        """ë¸”ë¡œê·¸ ì „ì²´ ë­í¬ ê³„ì‚°"""
        if not post_results:
            return "F", "Fë“±ê¸‰ (ë¶„ì„ ë¶ˆê°€)"
        
        # í‰ê·  ì ìˆ˜
        avg_score = sum(p['total_score'] for p in post_results) / len(post_results)
        
        # ë¸”ë¡œê·¸ ë­í¬
        if avg_score >= 90:
            blog_rank = "S"
        elif avg_score >= 80:
            blog_rank = "A"
        elif avg_score >= 70:
            blog_rank = "B"
        elif avg_score >= 60:
            blog_rank = "C"
        elif avg_score >= 50:
            blog_rank = "D"
        else:
            blog_rank = "F"
        
        # ì˜ˆìƒ ìœ ì… ë­í¬
        if avg_score >= 90:
            traffic_rank = "Së“±ê¸‰ (ë§¤ìš° ë†’ìŒ)"
        elif avg_score >= 80:
            traffic_rank = "Aë“±ê¸‰ (ë†’ìŒ)"
        elif avg_score >= 70:
            traffic_rank = "Bë“±ê¸‰ (ë³´í†µ)"
        elif avg_score >= 60:
            traffic_rank = "Cë“±ê¸‰ (ë‚®ìŒ)"
        elif avg_score >= 50:
            traffic_rank = "Dë“±ê¸‰ (ë§¤ìš° ë‚®ìŒ)"
        else:
            traffic_rank = "Fë“±ê¸‰ (ê¸°ëŒ€ ì–´ë ¤ì›€)"
        
        return blog_rank, traffic_rank

    async def _analyze_hashtag_competition(self, post_results: List[Dict]) -> Optional[Dict]:
        """
        ê²Œì‹œê¸€ì˜ í•´ì‹œíƒœê·¸ ê²½ìŸë ¥ ë¶„ì„

        Args:
            post_results: ê²Œì‹œê¸€ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í•´ì‹œíƒœê·¸ ê²½ìŸë ¥ ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. ëª¨ë“  í•´ì‹œíƒœê·¸ ìˆ˜ì§‘
            all_hashtags = []
            for post in post_results:
                hashtag_count = post.get('hashtag_count', 0)
                if hashtag_count > 0:
                    # ì‹¤ì œ í•´ì‹œíƒœê·¸ ë¦¬ìŠ¤íŠ¸ëŠ” post_dataì— ìˆì„ ìˆ˜ ìˆìŒ
                    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìˆ˜ì§‘
                    all_hashtags.extend([''] * hashtag_count)

            # 2. ë¹ˆë„ìˆ˜ ê³„ì‚° ë° ìƒìœ„ ì„ ì •
            # ì‹¤ì œ í•´ì‹œíƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ ë‹¤ì‹œ í¬ë¡¤ë§ í•„ìš”
            # ê°„ë‹¨íˆ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_to_analyze = set()

            for post in post_results:
                title = post.get('title', '')
                # ì œëª©ì—ì„œ 2ê¸€ì ì´ìƒ ë‹¨ì–´ ì¶”ì¶œ
                words = re.findall(r'[ê°€-í£]{2,}', title)
                keywords_to_analyze.update(words[:3])  # ìƒìœ„ 3ê°œë§Œ

            keywords_to_analyze = list(keywords_to_analyze)[:10]  # ìµœëŒ€ 10ê°œ

            if not keywords_to_analyze:
                return None

            print(f"ğŸ” í‚¤ì›Œë“œ ê²½ìŸë ¥ ë¶„ì„ ì¤‘: {keywords_to_analyze}")

            # 3. í‚¤ì›Œë“œ ê²½ìŸë ¥ ë¶„ì„ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ì—ì„œ í˜¸ì¶œ)
            keyword_results = []
            for keyword in keywords_to_analyze:
                try:
                    result = self.keyword_api.analyze_keyword_competition(keyword)
                    if result:
                        keyword_results.append(result)
                    # API ì œí•œ ê³ ë ¤
                    await asyncio.sleep(0.3)
                except Exception as e:
                    print(f"âš ï¸ '{keyword}' ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue

            if not keyword_results:
                return None

            # 4. ê²°ê³¼ ìš”ì•½
            total_search = sum(k['total_monthly_search'] for k in keyword_results)
            avg_competition_score = sum(k['competition_score'] for k in keyword_results) / len(keyword_results)

            # ì¶”ì²œ í‚¤ì›Œë“œ (ê²½ìŸ ë‚®ê³  ê²€ìƒ‰ëŸ‰ ìˆëŠ” ê²ƒ)
            recommended_keywords = [
                k for k in keyword_results
                if k.get('recommended', False)
            ]

            # ê²½ìŸ ë†’ì€ í‚¤ì›Œë“œ
            high_competition_keywords = [
                k for k in keyword_results
                if k['competition'] == 'ë†’ìŒ'
            ]

            analysis_result = {
                "total_keywords_analyzed": len(keyword_results),
                "total_monthly_search": total_search,
                "avg_competition_score": round(avg_competition_score, 1),
                "keywords": keyword_results,
                "recommended_keywords": recommended_keywords[:5],  # ìƒìœ„ 5ê°œ
                "high_competition_keywords": high_competition_keywords[:5],
                "analysis_summary": self._generate_keyword_summary(keyword_results)
            }

            return analysis_result

        except Exception as e:
            print(f"âŒ í•´ì‹œíƒœê·¸ ê²½ìŸë ¥ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None

    def _generate_keyword_summary(self, keyword_results: List[Dict]) -> str:
        """í‚¤ì›Œë“œ ë¶„ì„ ìš”ì•½ ìƒì„±"""
        if not keyword_results:
            return "ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤."

        avg_score = sum(k['competition_score'] for k in keyword_results) / len(keyword_results)
        recommended_count = sum(1 for k in keyword_results if k.get('recommended', False))

        if avg_score >= 70:
            summary = f"âš ï¸ ë†’ì€ ê²½ìŸ: ì‚¬ìš© ì¤‘ì¸ í‚¤ì›Œë“œë“¤ì˜ ê²½ìŸì´ ì¹˜ì—´í•©ë‹ˆë‹¤. (í‰ê·  {avg_score:.1f}ì )"
        elif avg_score >= 50:
            summary = f"âš¡ ì¤‘ê°„ ê²½ìŸ: ì ì ˆí•œ ìˆ˜ì¤€ì˜ ê²½ìŸ í‚¤ì›Œë“œì…ë‹ˆë‹¤. (í‰ê·  {avg_score:.1f}ì )"
        else:
            summary = f"âœ… ë‚®ì€ ê²½ìŸ: ê²½ìŸì´ ë‚®ì€ ë¸”ë£¨ì˜¤ì…˜ í‚¤ì›Œë“œì…ë‹ˆë‹¤. (í‰ê·  {avg_score:.1f}ì )"

        if recommended_count > 0:
            summary += f" | ì¶”ì²œ í‚¤ì›Œë“œ {recommended_count}ê°œ ë°œê²¬"

        return summary
