#!/usr/bin/env python3
"""
SEO ìµœì í™” ì—”ì§„

SEO ì„¤ì •ì— ë§ê²Œ ì›ê³  ìµœì í™”
"""

import os
import re
from typing import Optional, Dict, List
import google.generativeai as genai

from seo_config import SEOConfig
from keyword_analyzer import KeywordAnalyzer
from particle_handler import ParticleHandler
from forbidden_words_loader import ForbiddenWordsLoader


class SEOOptimizer:
    """SEO ê¸°ì¤€ì— ë§ê²Œ ì›ê³  ìµœì í™”"""

    def __init__(self, api_key: Optional[str] = None, forbidden_words_file: str = 'ê¸ˆì¹™ì–´ ë¦¬ìŠ¤íŠ¸.xlsx'):
        """
        ì´ˆê¸°í™”

        Args:
            api_key: Gemini API í‚¤
            forbidden_words_file: ê¸ˆì¹™ì–´ ë¦¬ìŠ¤íŠ¸ Excel íŒŒì¼
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.analyzer = KeywordAnalyzer()
        self.particle_handler = ParticleHandler(self.api_key)
        self.forbidden_loader = ForbiddenWordsLoader(forbidden_words_file)

        # Gemini ì´ˆê¸°í™”
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.5-pro')
        else:
            print("âš ï¸ Gemini API í‚¤ ì—†ìŒ - AI ê¸°ëŠ¥ ì œí•œë¨")
            self.model = None

    def _fix_particles(self, text: str, keyword: str) -> str:
        """
        ì¡°ì‚¬ ì²˜ë¦¬ (í†µ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ ê°€ëŠ¥í•˜ê²Œ)

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        return self.particle_handler.fix_all_particles(text, keyword)

    def _adjust_whole_keyword_count(self, text: str, config: SEOConfig) -> str:
        """
        í†µ í‚¤ì›Œë“œ ì¶œí˜„ íšŸìˆ˜ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        keyword = config.whole_keyword
        target_count = config.whole_keyword_count
        current_count = self.analyzer.count_whole_keyword(text, keyword)

        print(f"\ní†µ í‚¤ì›Œë“œ ì¡°ì •: '{keyword}'")
        print(f"  í˜„ì¬: {current_count}íšŒ â†’ ëª©í‘œ: {target_count}íšŒ")

        if current_count == target_count:
            print(f"  âœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±!")
            return text

        if current_count < target_count:
            # í‚¤ì›Œë“œ ì¶”ê°€ í•„ìš”
            print(f"  â• {target_count - current_count}íšŒ ì¶”ê°€ í•„ìš”")
            text = self._add_whole_keyword(text, keyword, target_count - current_count)
        else:
            # í‚¤ì›Œë“œ ê°ì†Œ í•„ìš”
            print(f"  â– {current_count - target_count}íšŒ ê°ì†Œ í•„ìš”")
            text = self._reduce_whole_keyword(text, keyword, current_count - target_count)

        return text

    def _add_whole_keyword(self, text: str, keyword: str, count: int) -> str:
        """
        í†µ í‚¤ì›Œë“œ ì¶”ê°€ (AI í™œìš©)

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ
            count: ì¶”ê°€í•  íšŸìˆ˜

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        if not self.model:
            print("  âš ï¸ AI ì—†ìŒ - í‚¤ì›Œë“œ ì¶”ê°€ ë¶ˆê°€")
            return text

        prompt = f"""ë‹¹ì‹ ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¸”ë¡œê·¸ ê¸€ í¸ì§‘ìì…ë‹ˆë‹¤.

# ìš”ì²­

ì•„ë˜ ì›ê³ ì— "{keyword}" í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ {count}íšŒ ì¶”ê°€í•˜ì„¸ìš”.

**ì œì•½ ì¡°ê±´:**
1. ì›ë³¸ ê¸€ì˜ êµ¬ì¡°ì™€ ì˜ë¯¸ ìœ ì§€
2. í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•¨ (ì¡°ì‚¬ ë¶™ì´ì§€ ë§ ê²ƒ)
3. ì–µì§€ë¡œ ë„£ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ
4. ë¸”ë¡œê·¸ ë§íˆ¬ ìœ ì§€

**ì›ê³ :**
{text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ì¶œë ¥
ìˆ˜ì •ëœ ì›ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´.
"""

        try:
            response = self.model.generate_content(prompt)
            if response.text:
                return response.text.strip()
        except Exception as e:
            print(f"  âš ï¸ AI ì˜¤ë¥˜: {e}")

        return text

    def _reduce_whole_keyword(self, text: str, keyword: str, count: int) -> str:
        """
        í†µ í‚¤ì›Œë“œ ê°ì†Œ

        Args:
            text: ì›ê³ 
            keyword: í†µ í‚¤ì›Œë“œ
            count: ê°ì†Œí•  íšŸìˆ˜

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        # í†µ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ì„œ ë’¤ì—ì„œë¶€í„° ì œê±° (ì•ìª½ í‚¤ì›Œë“œ ë³´ì¡´)
        # íŒ¨í„´: í‚¤ì›Œë“œ ë’¤ì— ê³µë°±ì´ë‚˜ ë¬¸ì¥ë¶€í˜¸
        pattern = re.escape(keyword) + r'(?=\s|[.,!?;:\)\]\}]|$)'

        matches = list(re.finditer(pattern, text))

        if len(matches) <= count:
            print(f"  âš ï¸ ì œê±°í•  í‚¤ì›Œë“œê°€ ë¶€ì¡±í•¨")
            return text

        # ë’¤ì—ì„œë¶€í„° ì œê±°
        removed = 0
        for match in reversed(matches):
            if removed >= count:
                break

            # í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ê³  ë¬¸ë§¥ì— ë§ëŠ” ëŒ€ëª…ì‚¬ë¡œ êµì²´
            start, end = match.span()
            before = text[:start]
            after = text[end:]

            # ê°„ë‹¨íˆ "ì´ê²ƒ"ìœ¼ë¡œ êµì²´
            text = before + "ì´ê²ƒ" + after
            removed += 1

        return text

    def _adjust_piece_keywords(self, text: str, config: SEOConfig) -> str:
        """
        ì¡°ê° í‚¤ì›Œë“œ ì¶œí˜„ íšŸìˆ˜ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        if not config.piece_keywords:
            return text

        print(f"\nì¡°ê° í‚¤ì›Œë“œ ì¡°ì •:")

        for piece, target_count in config.piece_keywords.items():
            current_count = self.analyzer.count_piece_keywords(text, [piece])[piece]
            print(f"  '{piece}': {current_count}íšŒ â†’ {target_count}íšŒ")

            if current_count == target_count:
                print(f"    âœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±!")
                continue

            # TODO: ì¡°ê° í‚¤ì›Œë“œ ì¶”ê°€/ê°ì†Œ ë¡œì§
            # í†µ í‚¤ì›Œë“œë³´ë‹¤ ë³µì¡í•¨ - ë‚˜ì¤‘ì— êµ¬í˜„

        return text

    def _adjust_first_paragraph(self, text: str, config: SEOConfig) -> str:
        """
        ì²« ë¬¸ë‹¨ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        keyword = config.whole_keyword

        first_para = self.analyzer.get_first_paragraph(text)
        first_para_count = self.analyzer.count_whole_keyword(first_para, keyword)
        sentences_between = self.analyzer.count_sentences_between_keywords(first_para, keyword)

        print(f"\nì²« ë¬¸ë‹¨ ì¡°ì •:")
        print(f"  í‚¤ì›Œë“œ ì¶œí˜„: {first_para_count}íšŒ")

        # 1. ì²« ë¬¸ë‹¨ í‚¤ì›Œë“œ 2íšŒ ì²´í¬
        if config.first_para_keyword_twice:
            if first_para_count < 2:
                print(f"  â• ì²« ë¬¸ë‹¨ì— í‚¤ì›Œë“œ {2 - first_para_count}íšŒ ë” í•„ìš”")
                # TODO: AIë¡œ í‚¤ì›Œë“œ ì¶”ê°€
            elif first_para_count > 2:
                print(f"  â– ì²« ë¬¸ë‹¨ì— í‚¤ì›Œë“œ {first_para_count - 2}íšŒ ì œê±° í•„ìš”")
                # TODO: í‚¤ì›Œë“œ ì œê±°
            else:
                print(f"  âœ… ì²« ë¬¸ë‹¨ í‚¤ì›Œë“œ 2íšŒ ì¶©ì¡±!")

        # 2. í‚¤ì›Œë“œ ì‚¬ì´ 2ë¬¸ì¥ ì²´í¬
        if config.first_para_two_sentences_between:
            if sentences_between < 2:
                print(f"  â• í‚¤ì›Œë“œ ì‚¬ì´ {2 - sentences_between}ê°œ ë¬¸ì¥ ë” í•„ìš”")
                # TODO: AIë¡œ ë¬¸ì¥ ì¶”ê°€
            elif sentences_between > 2:
                print(f"  â– í‚¤ì›Œë“œ ì‚¬ì´ {sentences_between - 2}ê°œ ë¬¸ì¥ ì œê±° í•„ìš”")
                # TODO: ë¬¸ì¥ ì œê±°
            else:
                print(f"  âœ… í‚¤ì›Œë“œ ì‚¬ì´ 2ë¬¸ì¥ ì¶©ì¡±!")

        return text

    def _adjust_starting_sentences(self, text: str, config: SEOConfig) -> str:
        """
        í†µ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        keyword = config.whole_keyword
        target_count = config.sentences_start_with_keyword
        current_count = self.analyzer.find_sentences_starting_with_keyword(text, keyword)

        print(f"\ní‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥:")
        print(f"  í˜„ì¬: {current_count}ê°œ â†’ ëª©í‘œ: {target_count}ê°œ")

        if current_count == target_count:
            print(f"  âœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±!")
        else:
            # TODO: AIë¡œ ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ ìˆ˜ì •
            print(f"  âš ï¸ êµ¬í˜„ í•„ìš”")

        return text

    def _adjust_sub_keywords(self, text: str, config: SEOConfig) -> str:
        """
        ì„œë¸Œ í‚¤ì›Œë“œ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        keyword = config.whole_keyword
        pieces = config.piece_keywords.keys() if config.piece_keywords else []
        exclude = [keyword] + list(pieces)

        current_count = self.analyzer.count_sub_keywords(text, exclude)
        target_count = config.sub_keyword_count

        print(f"\nì„œë¸Œ í‚¤ì›Œë“œ:")
        print(f"  í˜„ì¬: {current_count}ê°œ â†’ ëª©í‘œ: {target_count}ê°œ")

        if current_count == target_count:
            print(f"  âœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±!")
        else:
            # TODO: ì„œë¸Œ í‚¤ì›Œë“œ ì¡°ì • (ë³µì¡í•¨ - ë‚˜ì¤‘ì—)
            print(f"  âš ï¸ êµ¬í˜„ í•„ìš”")

        return text

    def _adjust_char_count(self, text: str, config: SEOConfig) -> str:
        """
        ê¸€ììˆ˜ ì¡°ì •

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        current_count = len(text)
        target_count = config.char_count

        print(f"\nê¸€ììˆ˜:")
        print(f"  í˜„ì¬: {current_count}ì â†’ ëª©í‘œ: {target_count}ì")

        # Â±10% í—ˆìš© ë²”ìœ„
        tolerance = target_count * 0.1
        if abs(current_count - target_count) <= tolerance:
            print(f"  âœ… í—ˆìš© ë²”ìœ„ ë‚´!")
            return text

        if current_count < target_count:
            print(f"  â• {target_count - current_count}ì ì¶”ê°€ í•„ìš”")
            # TODO: AIë¡œ ë‚´ìš© í™•ì¥
        else:
            print(f"  â– {current_count - target_count}ì ê°ì†Œ í•„ìš”")
            # TODO: AIë¡œ ë‚´ìš© ì¶•ì†Œ

        return text

    def _apply_forbidden_words(self, text: str) -> str:
        """
        ê¸ˆì¹™ì–´ ì¹˜í™˜

        Args:
            text: ì›ê³ 

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        print(f"\nê¸ˆì¹™ì–´ ì¹˜í™˜:")

        replacements = self.forbidden_loader.get_replacements()
        replaced_count = 0

        for forbidden, alternatives in replacements.items():
            if forbidden in text:
                # ì²« ë²ˆì§¸ ëŒ€ì²´ì–´ ì‚¬ìš©
                replacement = alternatives[0] if alternatives else forbidden
                text = text.replace(forbidden, replacement)
                replaced_count += 1
                print(f"  '{forbidden}' â†’ '{replacement}'")

        print(f"  ì´ {replaced_count}ê°œ ì¹˜í™˜ ì™„ë£Œ")

        return text

    def _ai_polish(self, text: str, config: SEOConfig) -> str:
        """
        AIë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ê¸°

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            ìˆ˜ì •ëœ ì›ê³ 
        """
        if not self.model:
            print("\nâš ï¸ AI ì—†ìŒ - ë‹¤ë“¬ê¸° ìƒëµ")
            return text

        print(f"\nğŸ¤– AI ë‹¤ë“¬ê¸° ì¤‘...")

        keyword = config.whole_keyword

        prompt = f"""ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ ê¸€ì˜ ì–´ìƒ‰í•œ ë¶€ë¶„ë§Œ ì‚´ì§ ê³ ì¹˜ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.

âš ï¸ **í•µì‹¬ ì›ì¹™: ì›ë³¸ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”!**

# ì…ë ¥ ì›ê³ 
í‚¤ì›Œë“œ: {keyword}

{text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ìš”ì²­

**ì–´ìƒ‰í•œ ë¶€ë¶„ë§Œ ìµœì†Œí•œìœ¼ë¡œ ìˆ˜ì •:**
1. ì¡°ì‚¬ ì˜¤ë¥˜ ìˆ˜ì •
2. ì¤‘ë³µ í‘œí˜„ ì œê±°
3. ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ êµì²´
4. ì›ë³¸ êµ¬ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€

**ì¶œë ¥:**
ìˆ˜ì •ëœ ì›ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´.
"""

        try:
            response = self.model.generate_content(prompt)
            if response.text:
                print(f"  âœ… ë‹¤ë“¬ê¸° ì™„ë£Œ")
                return response.text.strip()
        except Exception as e:
            print(f"  âš ï¸ AI ì˜¤ë¥˜: {e}")

        return text

    def optimize(self, text: str, config: SEOConfig) -> Dict:
        """
        SEO ì„¤ì •ì— ë§ê²Œ ì›ê³  ìµœì í™”

        Args:
            text: ì›ê³ 
            config: SEO ì„¤ì •

        Returns:
            {
                'optimized_text': ìµœì í™”ëœ ì›ê³ ,
                'analysis': ë¶„ì„ ê²°ê³¼
            }
        """
        print(f"\n{'='*80}")
        print(f"SEO ìµœì í™” ì‹œì‘")
        print(f"{'='*80}")
        print(config)
        print(f"{'='*80}\n")

        # 1. ì¡°ì‚¬ ì²˜ë¦¬ (í†µ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ ê°€ëŠ¥í•˜ê²Œ)
        text = self._fix_particles(text, config.whole_keyword)

        # 2. í†µ í‚¤ì›Œë“œ íšŸìˆ˜ ì¡°ì •
        text = self._adjust_whole_keyword_count(text, config)

        # 3. ì¡°ê° í‚¤ì›Œë“œ íšŸìˆ˜ ì¡°ì •
        text = self._adjust_piece_keywords(text, config)

        # 4. ì²« ë¬¸ë‹¨ ì¡°ì •
        text = self._adjust_first_paragraph(text, config)

        # 5. í†µ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ì¡°ì •
        text = self._adjust_starting_sentences(text, config)

        # 6. ì„œë¸Œ í‚¤ì›Œë“œ ì¡°ì •
        text = self._adjust_sub_keywords(text, config)

        # 7. ê¸€ììˆ˜ ì¡°ì •
        text = self._adjust_char_count(text, config)

        # 8. ê¸ˆì¹™ì–´ ì¹˜í™˜
        if config.apply_forbidden_words:
            text = self._apply_forbidden_words(text)

        # 9. AI ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ê¸°
        text = self._ai_polish(text, config)

        # ìµœì¢… ë¶„ì„
        analysis = self.analyzer.analyze(text, config.whole_keyword)

        print(f"\n{'='*80}")
        print(f"SEO ìµœì í™” ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"í†µ í‚¤ì›Œë“œ: {analysis['whole_keyword_count']}íšŒ (ëª©í‘œ: {config.whole_keyword_count}íšŒ)")
        print(f"ì¡°ê° í‚¤ì›Œë“œ: {analysis['piece_keyword_counts']}")
        print(f"ì„œë¸Œ í‚¤ì›Œë“œ: {analysis['sub_keyword_count']}ê°œ (ëª©í‘œ: {config.sub_keyword_count}ê°œ)")
        print(f"í‚¤ì›Œë“œë¡œ ì‹œì‘ ë¬¸ì¥: {analysis['sentences_start_with_keyword']}ê°œ (ëª©í‘œ: {config.sentences_start_with_keyword}ê°œ)")
        print(f"ê¸€ììˆ˜: {analysis['char_count']}ì (ëª©í‘œ: {config.char_count}ì)")
        print(f"{'='*80}\n")

        return {
            'optimized_text': text,
            'analysis': analysis,
        }


def test_seo_optimizer():
    """í…ŒìŠ¤íŠ¸"""

    # API í‚¤ í™•ì¸
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("âŒ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    optimizer = SEOOptimizer()

    # í…ŒìŠ¤íŠ¸ ì›ê³ 
    test_text = """ê°•ë‚¨ ë§›ì§‘ì„ ì°¾ê³  ìˆì–´ìš”.
ìš”ì¦˜ íšŒì‹ ì¥ì†Œë¡œ ê°•ë‚¨ì—ì„œ ë§›ì§‘ ì°¾ëŠ” ì¤‘ì¸ë°ìš”.
ê°•ë‚¨ ë§›ì§‘ ì¶”ì²œ ë°›ê³  ì‹¶ì–´ì„œ ê¸€ ì˜¬ë ¤ìš”.

íšŒì‹ ì¥ì†Œë¡œ ì¢‹ì€ ê³³ ìˆìœ¼ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.
ì •ë§ ê¶ê¸ˆí•´ìš”^^"""

    # SEO ì„¤ì •
    config = SEOConfig(
        whole_keyword="ê°•ë‚¨ ë§›ì§‘",
        whole_keyword_count=5,
        piece_keywords={"ê°•ë‚¨": 10, "ë§›ì§‘": 4},
        char_count=1000,
        apply_forbidden_words=False,
        first_para_keyword_twice=True,
        first_para_two_sentences_between=True,
        sub_keyword_count=20,
        sentences_start_with_keyword=6,
    )

    # ìµœì í™”
    result = optimizer.optimize(test_text, config)

    print("\n" + "=" * 80)
    print("ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    print(result['optimized_text'])
    print("=" * 80)


if __name__ == '__main__':
    test_seo_optimizer()
